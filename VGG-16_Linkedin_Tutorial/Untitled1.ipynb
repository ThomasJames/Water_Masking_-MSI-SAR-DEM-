{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1036878703ac41f1b763ee186d727383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_94aef9cef96c4a179a508633704d2fef",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_73bee376653f4a28ac990d16fbf5ecbb",
              "IPY_MODEL_59e5d7cd8f5c4887b85c4d25a2bcf085"
            ]
          }
        },
        "94aef9cef96c4a179a508633704d2fef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "73bee376653f4a28ac990d16fbf5ecbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4de6857bf5b1455eaf36e1054f396329",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a08ef01b90ea4bf6bd87b48c1769b0d6"
          }
        },
        "59e5d7cd8f5c4887b85c4d25a2bcf085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c2cd38afd2de4999906ceeb3872b36e3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [15:17&lt;00:00, 603kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_81cb2c639a0c4d2dbf56cd5c362551bc"
          }
        },
        "4de6857bf5b1455eaf36e1054f396329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a08ef01b90ea4bf6bd87b48c1769b0d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c2cd38afd2de4999906ceeb3872b36e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "81cb2c639a0c4d2dbf56cd5c362551bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTt5KEJqv7JB",
        "colab_type": "text"
      },
      "source": [
        "Attempting to apply transfer learning to train a VGG-16 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPm8qNAynU65",
        "colab_type": "text"
      },
      "source": [
        "Importing all the modules needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhhPs2bDjDqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-BOIbmdncTy",
        "colab_type": "text"
      },
      "source": [
        "Load in the VGG-16 network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F2D17hBna8D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "1036878703ac41f1b763ee186d727383",
            "94aef9cef96c4a179a508633704d2fef",
            "73bee376653f4a28ac990d16fbf5ecbb",
            "59e5d7cd8f5c4887b85c4d25a2bcf085",
            "4de6857bf5b1455eaf36e1054f396329",
            "a08ef01b90ea4bf6bd87b48c1769b0d6",
            "c2cd38afd2de4999906ceeb3872b36e3",
            "81cb2c639a0c4d2dbf56cd5c362551bc"
          ]
        },
        "outputId": "e25c4abb-5b86-4d5b-be80-33ad040c614f"
      },
      "source": [
        "vgg16 = models.vgg16(pretrained=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1036878703ac41f1b763ee186d727383",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CBs2fktpjS5",
        "colab_type": "text"
      },
      "source": [
        "Look at the features and the classifier of the VGG-16 network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCWWoXkWpg_z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "f13795fa-141f-422e-bb1e-323a59fc074c"
      },
      "source": [
        "print(vgg16.features)\n",
        "print(vgg16.classifier)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (18): ReLU(inplace=True)\n",
            "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (25): ReLU(inplace=True)\n",
            "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (27): ReLU(inplace=True)\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n",
            "Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX2HZ2Sfp1t7",
        "colab_type": "text"
      },
      "source": [
        "Freeze the network: Grab all the parameters, and set the requires grad to False."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mn4zPZsp4c2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in vgg16.parameters():\n",
        "  param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN62lI_sqN0Z",
        "colab_type": "text"
      },
      "source": [
        "Remove the last fully connected layer, and treat the rest of the network as a fixed feature eztractor.\n",
        "We then add a linear classifier, like logsoftmax to train on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNnH-8hRrBIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg16.classifier[-1] = nn.Sequential(\n",
        "    nn.Linear(in_features=4096, out_features=2),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiGy7gkyrkDK",
        "colab_type": "text"
      },
      "source": [
        "Now look again at the classifer to see what has changed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCBHGsuxroR0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8c2d8984-d344-42e6-c800-e4f8f7ea4d1f"
      },
      "source": [
        "print(vgg16.classifier)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Sequential(\n",
            "    (0): Linear(in_features=4096, out_features=2, bias=True)\n",
            "    (1): LogSoftmax()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2pvOlxHrt84",
        "colab_type": "text"
      },
      "source": [
        "Set the criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loE6vfQ8sAVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.Sigmoid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-pEzEOXsFW-",
        "colab_type": "text"
      },
      "source": [
        "The model doesn't now the difference between urban water and land.\n",
        "VGG-16 model can be retrained to be able to determine the difference if its given lots of examples.\n",
        "The loss function takes in what the model predicts, and its label, then computes a value for how far away the prediction is from the actual value. \n",
        "\n",
        "The task is a Binary Classification task, therefore the best loss function is the binary crossentropy. (And sigmoid should be the Loss function of the last year.)\n",
        "\n",
        "To minimise the criterion we have to adjust the parameters of the network. \n",
        "We have to adjust the weights in the network. \n",
        "\n",
        "All of the weights in the network have been frozen except for the last fully connected layer - The goal is to train these weights. \n",
        "\n",
        "This is achieved by using gradient descent, in PyTorch this is achieved using Autograd. \n",
        "\n",
        "The loss functoin is differentiable with respect to the loss.\n",
        "\n",
        "In the forward pass we calculate the loss, then in the backward pass we calculate the gradient of the loss with respect to each of the weights. \n",
        "\n",
        "note\n",
        "Set .requires_grad to True  - Essentially the same as unfreezing the network\n",
        "call .backward() - Automatically computes all the gradients. \n",
        "\n",
        "This only has to be done during the training phase of the model.\n",
        "\n",
        "Once trained, we turn autograd off. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TqpU-gfsZpd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eedbd73c-7f51-415a-c494-0ec3959e45db"
      },
      "source": [
        "print(\" \")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaqAaUyxvT9q",
        "colab_type": "text"
      },
      "source": [
        "Using Autograd.\n",
        "\n",
        "Autograd is used to automatically calculate the gradients of tensors \n",
        "- To make sure the network keeps track of the gradients, we need to set \n",
        ".requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkOw2hFLvkC7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cb230895-61d3-4c9b-abae-ad47b2d4159e"
      },
      "source": [
        "w = torch.randn(4, 3, requires_grad=True)\n",
        "w"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1350, -1.1881, -0.0651],\n",
              "        [-0.8247, -1.3694, -0.2737],\n",
              "        [ 1.9208,  2.2549,  0.3004],\n",
              "        [-0.5083, -0.1567, -0.0266]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uxinDz4wTP3",
        "colab_type": "text"
      },
      "source": [
        "freeze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWOY_0MOvuaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ede86a7d-1a17-4951-c38d-d2f94e17b9ce"
      },
      "source": [
        "w.requires_grad_(False)\n",
        "w"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1350, -1.1881, -0.0651],\n",
              "        [-0.8247, -1.3694, -0.2737],\n",
              "        [ 1.9208,  2.2549,  0.3004],\n",
              "        [-0.5083, -0.1567, -0.0266]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtqpsCH4wPlL",
        "colab_type": "text"
      },
      "source": [
        "Unfreeze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFOL5aqlwJeu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "09c5f1c6-3be5-42a7-b4ce-627c0a45f6ad"
      },
      "source": [
        "w.requires_grad_(True)\n",
        "w"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1350, -1.1881, -0.0651],\n",
              "        [-0.8247, -1.3694, -0.2737],\n",
              "        [ 1.9208,  2.2549,  0.3004],\n",
              "        [-0.5083, -0.1567, -0.0266]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi7dfrCPwWie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "95b16cab-0fcc-4f4d-c99f-18b37575c7bd"
      },
      "source": [
        "y = torch.exp(w)\n",
        "print(y)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.8737, 0.3048, 0.9370],\n",
            "        [0.4384, 0.2543, 0.7606],\n",
            "        [6.8262, 9.5346, 1.3504],\n",
            "        [0.6015, 0.8549, 0.9737]], grad_fn=<ExpBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCpXM11_wjoQ",
        "colab_type": "text"
      },
      "source": [
        "Define an operation to get a scalar value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw6oR_hXwmzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d921bae-380d-4c31-fa12-45074351c3ec"
      },
      "source": [
        "output = y.mean()\n",
        "print(output)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.9758, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB95WP3wwyX1",
        "colab_type": "text"
      },
      "source": [
        "We can see that the gradients are currently empty.\n",
        "It returns none, because we haven't called the backward function yet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b99PRuS0wt0s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2c2bd95-99fc-4801-ae71-0ff8bb7cd4fa"
      },
      "source": [
        "print(w.grad)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_tPivYZxCP8",
        "colab_type": "text"
      },
      "source": [
        "So we do this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvPrf4b7xHCk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "042ad0c1-78f2-4e9b-fdd7-6497245d8f67"
      },
      "source": [
        "output.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-394d6a991a15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
          ]
        }
      ]
    }
  ]
}